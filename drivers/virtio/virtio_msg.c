// SPDX-License-Identifier: GPL-2.0+
/*
 * Virtio message transport.
 *
 * Copyright (C) 2024 Google LLC and Linaro.
 * Viresh Kumar <viresh.kumar@linaro.org>
 *
 * The Virtio message transport allows virtio devices to be used over a virtual
 * virtio-msg channel. The channel interface is meant to be implemented using
 * the architecture specific hardware-assisted fast path, like ARM Firmware
 * Framework (FFA).
 */

#define pr_fmt(fmt) "virtio-msg: " fmt

#include <linux/limits.h>
#include <linux/list.h>
#include <linux/miscdevice.h>
#include <linux/slab.h>
#include <linux/spinlock.h>
#include <linux/virtio.h>
#include <linux/virtio_config.h>
#include <linux/virtio_ring.h>
#include <uapi/linux/virtio_msg.h>

#include "virtio_msg.h"

#define U24_MAX					((1 << 24) - 1)
#define to_virtio_msg_device(_dev)		container_of(_dev, struct virtio_msg_device, vdev)
#define to_virtio_msg_user_device(_misc)	container_of(_misc, struct virtio_msg_user_device, misc)

static ssize_t vmsg_miscdev_read(struct file *file, char __user *buf,
				 size_t count, loff_t *pos)
{
	struct miscdevice *misc = file->private_data;
	struct virtio_msg_user_device *vmudev = to_virtio_msg_user_device(misc);
	int ret;

	if (count != VIRTIO_MSG_MAX_SIZE) {
		dev_err(vmudev->parent, "Trying to read message of incorrect size: %ld\n",
			count);
		return 0;
	}

	/* Wait to receive a message from the guest */
	ret = virtio_msg_async_wait(&vmudev->async, vmudev->parent, 0);
	if (ret < 0)
		return 0;

	BUG_ON(!vmudev->msg);

	/* The "msg" pointer is filled by the bus driver before waking up */
	if (copy_to_user(buf, vmudev->msg, count) != 0)
		return 0;

	vmudev->msg = NULL;

	return count;
}

static ssize_t vmsg_miscdev_write(struct file *file, const char __user *buf,
				  size_t count, loff_t *pos)
{
	struct miscdevice *misc = file->private_data;
	struct virtio_msg_user_device *vmudev = to_virtio_msg_user_device(misc);
	struct virtio_msg msg;

	if (count != VIRTIO_MSG_MAX_SIZE) {
		dev_err(vmudev->parent, "Trying to write message of incorrect size: %ld\n",
			count);
		return 0;
	}

	if (copy_from_user(&msg, buf, count) != 0)
		return 0;

	vmudev->ops->send(vmudev, &msg);

	return count;
}

static const struct file_operations vmsg_miscdev_fops = {
	.owner = THIS_MODULE,
	.read = vmsg_miscdev_read,
	.write = vmsg_miscdev_write,
};

int virtio_msg_user_register(struct virtio_msg_user_device *vmudev)
{
	static u8 vmsg_user_device_count = 0;
	int ret;

	if (!vmudev || !vmudev->ops)
		return -EINVAL;

	virtio_msg_async_init(&vmudev->async);

	vmudev->misc.parent = vmudev->parent;
	vmudev->misc.minor = MISC_DYNAMIC_MINOR;
	vmudev->misc.fops = &vmsg_miscdev_fops;
	vmudev->misc.name = vmudev->name;
	sprintf(vmudev->name, "virtio-msg-%d", vmsg_user_device_count);

	ret = misc_register(&vmudev->misc);
	if (ret)
		return ret;

	vmsg_user_device_count++;
	return 0;
}
EXPORT_SYMBOL_GPL(virtio_msg_user_register);

void virtio_msg_user_unregister(struct virtio_msg_user_device *vmudev)
{
	misc_deregister(&vmudev->misc);
}
EXPORT_SYMBOL_GPL(virtio_msg_user_unregister);

void virtio_msg_prepare(struct virtio_msg *msg, bool bus, u8 msg_id, u16 dev_id)
{
	/* Initialize all fields (including padding) to 0 */
	memset(msg, 0, sizeof(*msg));

	if (bus) {
		msg->type = VIRTIO_MSG_TYPE_BUS;
	} else {
		msg->type = VIRTIO_MSG_TYPE_VIRTIO;
		msg->dev_id = cpu_to_le16(dev_id);
	}

	msg->id = msg_id;
}
EXPORT_SYMBOL_GPL(virtio_msg_prepare);

static void vmsg_prepare(struct virtio_msg *msg, u8 msg_id, u16 dev_id)
{
	virtio_msg_prepare(msg, false, msg_id, dev_id);
}

static int vmsg_send(struct virtio_msg_device *vmdev,
		     struct virtio_msg *request, struct virtio_msg *response)
{
	return vmdev->ops->send(vmdev, request, response);
}

static int vmsg_get_device_info(struct virtio_msg_device *vmdev)
{
	struct virtio_msg request, response;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_DEVICE_INFO, vmdev->dev_id);

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0)
		return ret;

	vmdev->vdev.id.device = le32_to_cpu(response.get_device_info_resp.device_id);
	if (vmdev->vdev.id.device == 0) {
		/*
		 * virtio-msg device with an ID 0 is a (dummy) placeholder
		 * with no function.
		 */
		return -ENODEV;
	}

	vmdev->vdev.id.vendor = le32_to_cpu(response.get_device_info_resp.vendor_id);

	/* TODO: Device Version ? */
	return 0;
}

static u64 vmsg_get_features(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request, response;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_GET_FEATURES, vmdev->dev_id);
	request.get_features.index = 0;

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0)
		return ret;

	return le64_to_cpu(response.get_features_resp.features[0]);
}

static int vmsg_finalize_features(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request;
	int ret;

	/* Give virtio_ring a chance to accept features. */
	vring_transport_features(vdev);

	vmsg_prepare(&request, VIRTIO_MSG_SET_FEATURES, vmdev->dev_id);
	request.set_features.index = 0;
	request.set_features.features[0]= cpu_to_le64(vmdev->vdev.features);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0)
		return ret;

	return 0;
}

static void vmsg_get(struct virtio_device *vdev, unsigned int offset,
		   void *buf, unsigned int len)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request, response;
	u64 data;
	int ret;

	/* The protocol has reserved only 3 bytes for the offset */
	BUG_ON(offset > U24_MAX);
	BUG_ON(len > 8);

	vmsg_prepare(&request, VIRTIO_MSG_GET_CONFIG, vmdev->dev_id);

	offset = cpu_to_le32(offset);
	request.get_config.offset[0] = (u8) offset;
	request.get_config.offset[1] = (u8) (offset >> 8);
	request.get_config.offset[2] = (u8) (offset >> 16);
	request.get_config.size = (u8) len;

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0) {
		dev_err(&vdev->dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return;
	}

	data = le64_to_cpu(response.get_config_resp.data[0]);
	memcpy(buf, &data, len);
}

static void vmsg_set(struct virtio_device *vdev, unsigned int offset,
		   const void *buf, unsigned int len)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request;
	u64 data;
	int ret;

	/* The protocol has reserved only 3 bytes for the offset */
	BUG_ON(offset > U24_MAX);
	BUG_ON(len > 8);

	vmsg_prepare(&request, VIRTIO_MSG_SET_CONFIG, vmdev->dev_id);

	offset = cpu_to_le32(offset);
	request.set_config.offset[0] = (u8) offset;
	request.set_config.offset[1] = (u8) (offset >> 8);
	request.set_config.offset[2] = (u8) (offset >> 16);
	request.set_config.size = (u8) len;

	memcpy(&data, buf, len);
	request.set_config.data[0] = le64_to_cpu(data);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0) {
		dev_err(&vdev->dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return;
	}
}

static u32 vmsg_generation(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request, response;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_GET_CONFIG_GEN, vmdev->dev_id);

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0) {
		dev_err(&vdev->dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return 0;
	}

	return le32_to_cpu(response.get_config_gen_resp.generation);
}

static u8 vmsg_get_status(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request, response;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_GET_DEVICE_STATUS, vmdev->dev_id);

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0) {
		dev_err(&vdev->dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return 0;
	}

	return (u8) le32_to_cpu(response.get_device_status_resp.status);
}

static void vmsg_set_status(struct virtio_device *vdev, u8 status)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtio_msg request;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_SET_DEVICE_STATUS, vmdev->dev_id);
	request.set_device_status.status = cpu_to_le32(status);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0)
		dev_err(&vdev->dev, "%s: Failed to send request (%d)\n", __func__, ret);
}

static void vmsg_reset(struct virtio_device *vdev)
{
	/* 0 status means a reset. */
	vmsg_set_status(vdev, 0);
}

static bool vmsg_notify(struct virtqueue *vq)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vq->vdev);
	struct virtio_msg request;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_EVENT_AVAIL, vmdev->dev_id);
	request.event_avail.index = cpu_to_le32(vq->index);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0) {
		dev_err(&vmdev->vdev.dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return false;
	}

	return true;
}

static bool vmsg_notify_with_data(struct virtqueue *vq)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vq->vdev);
	u32 data = vring_notification_data(vq);
	struct virtio_msg request;
	int ret;

	vmsg_prepare(&request, VIRTIO_MSG_EVENT_AVAIL, vmdev->dev_id);
	request.event_avail.index = cpu_to_le32(data | 0xFFFF);
	data >>= 16;

	request.event_avail.next_offset = cpu_to_le32(data | 0x7FFF);
	request.event_avail.next_wrap = cpu_to_le32(data >> 15);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0) {
		dev_err(&vmdev->vdev.dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return false;
	}

	return true;
}

int virtio_msg_receive(struct virtio_msg_device *vmdev, struct virtio_msg *msg)
{
	struct virtio_msg_vq *info;
	bool handled = false;
	unsigned long flags;
	unsigned int index;
	int ret = 0;

	if (msg->id == VIRTIO_MSG_EVENT_CONFIG) {
		/*
		 * Though we have the config changes available here, the kernel
		 * implementation lets the driver fetch them again.
		 */
		virtio_config_changed(&vmdev->vdev);
	} else if (msg->id == VIRTIO_MSG_EVENT_USED) {
		index = le32_to_cpu(msg->event_used.index);

		spin_lock_irqsave(&vmdev->lock, flags);
		list_for_each_entry(info, &vmdev->virtqueues, node) {
			if (index == info->vq->index) {
				if (vring_interrupt(0, info->vq) != IRQ_HANDLED)
					ret = -EIO;

				handled = true;
				break;
			}
		}
		spin_unlock_irqrestore(&vmdev->lock, flags);

		if (!handled) {
			ret = -EINVAL;
			dev_err(&vmdev->vdev.dev, "%s: Failed to find virtqueue for message (%u)", __func__, index);
		}
	} else {
		dev_err(&vmdev->vdev.dev, "%s: Unexpected message id: (%u)\n", __func__, msg->id);
		ret = -EINVAL;
	}

	return ret;
}
EXPORT_SYMBOL_GPL(virtio_msg_receive);

static void vmsg_del_vq(struct virtqueue *vq)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vq->vdev);
	struct virtio_msg_vq *info = vq->priv;
	struct virtio_msg request;
	unsigned long flags;
	int ret;

	spin_lock_irqsave(&vmdev->lock, flags);
	list_del(&info->node);
	spin_unlock_irqrestore(&vmdev->lock, flags);

	/* Reset the virtqueue */
	vmsg_prepare(&request, VIRTIO_MSG_RESET_VQUEUE, vmdev->dev_id);
	request.reset_vqueue.index = cpu_to_le32(vq->index);

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0)
		dev_err(&vmdev->vdev.dev, "%s: Failed to send request (%d)\n", __func__, ret);

	vring_del_virtqueue(vq);

	kfree(info);
}

static void vmsg_del_vqs(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	struct virtqueue *vq, *n;

	list_for_each_entry_safe(vq, n, &vmdev->vdev.vqs, list)
		vmsg_del_vq(vq);

	if (vmdev->ops->release_vqs)
		vmdev->ops->release_vqs(vmdev);
}

static struct virtqueue *vmsg_setup_vq(struct virtio_msg_device *vmdev,
				       unsigned int index,
				       void (*callback)(struct virtqueue *vq),
				       const char *name, bool ctx)
{
	bool (*notify)(struct virtqueue *vq);
	struct virtio_msg_vq *info;
	struct virtio_msg request, response;
	struct virtqueue *vq;
	unsigned long flags;
	unsigned int num;
	int ret;

	if (__virtio_test_bit(&vmdev->vdev, VIRTIO_F_NOTIFICATION_DATA))
		notify = vmsg_notify_with_data;
	else
		notify = vmsg_notify;

	/* Get virtqueue max size from device */
	vmsg_prepare(&request, VIRTIO_MSG_GET_VQUEUE, vmdev->dev_id);
	request.get_vqueue.index = cpu_to_le32(index);

	ret = vmsg_send(vmdev, &request, &response);
	if (ret < 0) {
		dev_err(&vmdev->vdev.dev, "%s: Failed to send request (%d)\n", __func__, ret);
		return ERR_PTR(ret);
	}

	num = le32_to_cpu(response.get_vqueue_resp.max_size);
	if (!num)
		return ERR_PTR(-ENOENT);

	info = kmalloc(sizeof(*info), GFP_KERNEL);
	if (!info)
		return ERR_PTR(-ENOMEM);

	/* Create the vring */
	vq = vring_create_virtqueue(index, num, PAGE_SIZE, &vmdev->vdev, true,
				    true, ctx, notify, callback, name);
	if (!vq) {
		ret = -ENOMEM;
		goto free_info;
	}

	vq->num_max = num;

	/* Send virtqueue configuration to the device */
	vmsg_prepare(&request, VIRTIO_MSG_SET_VQUEUE, vmdev->dev_id);
	request.set_vqueue.index = cpu_to_le32(index);
	request.set_vqueue.size = cpu_to_le64(virtqueue_get_vring_size(vq));
	request.set_vqueue.descriptor_addr = cpu_to_le64(virtqueue_get_desc_addr(vq));
	request.set_vqueue.driver_addr = cpu_to_le64(virtqueue_get_avail_addr(vq));
	request.set_vqueue.device_addr = cpu_to_le64(virtqueue_get_used_addr(vq));

	ret = vmsg_send(vmdev, &request, NULL);
	if (ret < 0) {
		dev_err(&vmdev->vdev.dev, "%s: Failed to send request (%d)\n", __func__, ret);
		goto del_vq;
	}

	vq->priv = info;
	info->vq = vq;

	spin_lock_irqsave(&vmdev->lock, flags);
	list_add(&info->node, &vmdev->virtqueues);
	spin_unlock_irqrestore(&vmdev->lock, flags);

	return vq;

del_vq:
	vring_del_virtqueue(vq);
free_info:
	kfree(info);
	return ERR_PTR(ret);
}

static int vmsg_find_vqs(struct virtio_device *vdev, unsigned int nvqs,
		       struct virtqueue *vqs[],
		       struct virtqueue_info vqs_info[],
		       struct irq_affinity *desc)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);
	int i, ret, queue_idx = 0;

	if (vmdev->ops->prepare_vqs) {
		ret = vmdev->ops->prepare_vqs(vmdev);
		if (ret)
			return ret;
	}

	for (i = 0; i < nvqs; ++i) {
		struct virtqueue_info *vqi = &vqs_info[i];

		if (!vqi->name) {
			vqs[i] = NULL;
			continue;
		}

		vqs[i] = vmsg_setup_vq(vmdev, queue_idx++, vqi->callback,
				     vqi->name, vqi->ctx);
		if (IS_ERR(vqs[i])) {
			vmsg_del_vqs(vdev);
			return PTR_ERR(vqs[i]);
		}
	}

	return 0;
}

static const char *vmsg_bus_name(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);

	return vmdev->ops->bus_name(vmdev);
}

static void vmsg_synchronize_cbs(struct virtio_device *vdev)
{
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);

	vmdev->ops->synchronize_cbs(vmdev);
}

static void virtio_msg_release_dev(struct device *_d)
{
	struct virtio_device *vdev =
			container_of(_d, struct virtio_device, dev);
	struct virtio_msg_device *vmdev = to_virtio_msg_device(vdev);

	if (vmdev->ops->release)
		vmdev->ops->release(vmdev);
}

static struct virtio_config_ops virtio_msg_config_ops = {
	.get		= vmsg_get,
	.set		= vmsg_set,
	.generation	= vmsg_generation,
	.get_status	= vmsg_get_status,
	.set_status	= vmsg_set_status,
	.reset		= vmsg_reset,
	.find_vqs	= vmsg_find_vqs,
	.del_vqs	= vmsg_del_vqs,
	.get_features	= vmsg_get_features,
	.finalize_features = vmsg_finalize_features,
	.bus_name	= vmsg_bus_name,
};

int virtio_msg_register(struct virtio_msg_device *vmdev)
{
	int ret;

	/*
	 * Field expected to be filled by underlying architecture specific
	 * transport layer are vmdev->data (optional), vmdev->ops, and
	 * vmdev->vdev.dev.parent.
	 */
	if (!vmdev || !vmdev->ops || !vmdev->ops->send) {
		ret = -EINVAL;
		goto out;
	}

	virtio_msg_async_init(&vmdev->async);
	vmdev->vdev.config = &virtio_msg_config_ops;
	vmdev->vdev.dev.release = virtio_msg_release_dev;
	INIT_LIST_HEAD(&vmdev->virtqueues);
	spin_lock_init(&vmdev->lock);

	if (vmdev->ops->synchronize_cbs)
		virtio_msg_config_ops.synchronize_cbs = vmsg_synchronize_cbs;

	ret = vmsg_get_device_info(vmdev);
	if (ret)
		goto out;

	ret = register_virtio_device(&vmdev->vdev);
	if (ret)
		put_device(&vmdev->vdev.dev);

	return ret;

out:
	vmdev->ops->release(vmdev);
	return ret;
}
EXPORT_SYMBOL_GPL(virtio_msg_register);

void virtio_msg_unregister(struct virtio_msg_device *vmdev)
{
	unregister_virtio_device(&vmdev->vdev);
}
EXPORT_SYMBOL_GPL(virtio_msg_unregister);

MODULE_AUTHOR("Viresh Kumar <viresh.kumar@linaro.org>");
MODULE_DESCRIPTION("Virtio message transport");
MODULE_LICENSE("GPL");
